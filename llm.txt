#File ./conftest.py:
import sys
from pathlib import Path

# Add the src directory to the Python path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))







#File ./src/autocoder/nodes/tools/__init__.py:







#File ./src/autocoder/nodes/tools/directory_checker.py:
import os
import logging

logger = logging.getLogger(__name__)


def check_autocoder_dir():
    autocoder_dir = os.path.join(os.getcwd(), ".autocoder")
    project_state_file = os.path.join(autocoder_dir, "project_state.txt")
    return os.path.isdir(autocoder_dir) and os.path.isfile(project_state_file)


def display_init_message():
    message = """
Autocoder isn't initialized in this location. Please run "autocoder init" command for the autocoder to start working on files in this location.

Usage: autocoder [command]
The available commands for execution are listed below.
Commands:
  init          Init autocoder in this directory
  task          Execute a task in an initialized directory
  analyze       Analyze the project in an initialized directory
  help          Display help information
"""
    print(message)

def display_usage_message():
    message = """
Usage: autocoder [command]
The available commands for execution are listed below.
Commands:
  init          Init autocoder in this directory
  task          Execute a task in an initialized directory
  analyze       Analyze the project in an initialized directory
  help          Display help information
"""
    print(message)


def init_autocoder():
    autocoder_dir = os.path.join(os.getcwd(), ".autocoder")
    project_state_file = os.path.join(autocoder_dir, "project_state.txt")

    if not check_autocoder_dir():
        os.makedirs(autocoder_dir, exist_ok=True)
        with open(project_state_file, 'w') as f:
            f.write('initialized')
        logger.info("Autocoder initialized successfully in this directory with project state 'initialized'.")
        print("Autocoder initialized successfully in this directory with project state 'initialized'.")
    else:
        logger.info("Autocoder is already initialized in this directory.")
        print("Autocoder is already initialized in this directory.")







#File ./src/autocoder/nodes/__init__.py:







#File ./src/autocoder/nodes/apply_modifications.py:
from ..state import State
from ..code_modifier import CodeModifier
from ..file_manager import FileManager
from ..error_handler import ErrorHandler

@ErrorHandler.wrap_node
def apply_modifications(state: State) -> State:
    code_modifier = CodeModifier()
    file_manager = FileManager(state["project_root"])

    for file in state["interpreted_task"]["affected_files"]:
        if file in state["files"]:
            original_code = state["files"][file]
            modified_code = code_modifier.modify_code(original_code, state["modifications"])
            file_manager.write_file(file, modified_code)
            state["files"][file] = modified_code

    return state







#File ./src/autocoder/nodes/build_context.py:
from ..state import State
from ..context_builder import ContextBuilder
from ..file_manager import FileManager
from ..error_handler import ErrorHandler

@ErrorHandler.wrap_node
def build_context(state: State) -> State:
    file_manager = FileManager(state["project_root"])
    context_builder = ContextBuilder()

    files = file_manager.get_file_contents()
    context = context_builder.build_context(files)

    state["files"] = files
    state["context"] = context

    return state







#File ./src/autocoder/nodes/check_autocoder_dir.py:
from ..state import State
from ..nodes.tools.directory_checker import check_autocoder_dir as check_dir
from ..error_handler import ErrorHandler

@ErrorHandler.wrap_node
def check_autocoder_dir(state: State) -> State:
    state["autocoder_dir_exists"] = check_dir()
    return state







#File ./src/autocoder/nodes/error_handling_node.py:
from typing import Dict
from langchain_core.tools import Tool
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel

class ErrorHandlingArgs(BaseModel):
    error: str

def handle_error(state: Dict, args: ErrorHandlingArgs) -> Dict:
    # Implement error handling logic here
    state["error"] = f"Handled error: {args.error}"
    return state

error_handling_tools = [
    Tool.from_function(
        func=handle_error,
        name="handle_error",
        description="Handle errors in the workflow",
        args_schema=ErrorHandlingArgs
    )
]

error_handling_node = ToolNode(error_handling_tools)







#File ./src/autocoder/nodes/file_listing_node.py:
# File: src/autocoder/nodes/file_listing_node.py

import os
import logging
from pathlib import Path
import pathspec
from typing import Dict, List
from langchain_core.tools import Tool
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)

class FileListingArgs(BaseModel):
    project_root: str = Field(..., description="Root directory of the project")

class FileListingNode:
    @staticmethod
    def process(state: Dict, args: FileListingArgs) -> Dict:
        try:
            project_root = Path(args.project_root)
            ignore_spec = FileListingNode.get_ignore_spec(project_root)
            project_files = FileListingNode.list_project_files(project_root, ignore_spec)
            context = FileListingNode.build_context(project_root, project_files)

            state.update({
                'project_files': project_files,
                'excluded_files': [str(pat) for pat in ignore_spec.patterns],
                'context': context
            })

            logger.info(f"File listing completed. Found {len(project_files)} files.")
            return state
        except Exception as e:
            logger.error(f"Error in FileListingNode: {str(e)}")
            state['error'] = str(e)
            return state

    @staticmethod
    def get_ignore_spec(project_root: Path) -> pathspec.PathSpec:
        patterns = []
        gitignore_path = project_root / '.gitignore'
        if gitignore_path.exists():
            with gitignore_path.open('r') as f:
                patterns.extend(f.read().splitlines())
            logger.info("Read .gitignore and compiled ignore patterns.")
        else:
            logger.warning(".gitignore file not found.")

        default_ignores = [
            '.git/', '.hg/', '.svn/', '.idea/', '*.egg-info/',
            '__pycache__/', '.DS_Store', '*.pyc', '.venv/',
            'env/', 'build/', 'dist/', 'node_modules/',
            '*.log', '*.tmp',
        ]
        patterns.extend(default_ignores)
        return pathspec.PathSpec.from_lines('gitwildmatch', patterns)

    @staticmethod
    def list_project_files(project_root: Path, ignore_spec: pathspec.PathSpec) -> List[str]:
        project_files = []
        for root, dirs, files in os.walk(project_root):
            root_relative = os.path.relpath(root, project_root)
            if root_relative == '.':
                root_relative = ''
            dirs[:] = [d for d in dirs if not ignore_spec.match_file(os.path.join(root_relative, d) + '/')]
            for file in files:
                rel_path = os.path.join(root_relative, file)
                if not ignore_spec.match_file(rel_path):
                    project_files.append(rel_path)
        return project_files

    @staticmethod
    def build_context(project_root: Path, project_files: List[str]) -> str:
        context = ''
        for file in project_files:
            file_path = project_root / file
            try:
                with file_path.open('r', encoding='utf-8') as f:
                    content = f.read()
                context += f'#File {file}:\n{content}\n\n'
            except Exception as e:
                logger.warning(f"Could not read file {file}: {str(e)}")
        return context

def file_listing(state: Dict, args: FileListingArgs) -> Dict:
    return FileListingNode.process(state, args)

file_listing_tools = [
    Tool.from_function(
        func=file_listing,
        name="file_listing",
        description="List project files and build context",
        args_schema=FileListingArgs
    )
]

file_listing_node = ToolNode(file_listing_tools)







#File ./src/autocoder/nodes/generate_modifications.py:
from ..state import State
from ..task_interpreter import TaskInterpreter
from ..error_handler import ErrorHandler


@ErrorHandler.wrap_node
def generate_modifications(state: State) -> State:
    task_interpreter = TaskInterpreter()
    claude_api = state["claude_api"]

    task_prompt = task_interpreter.get_prompt_for_task(state["interpreted_task"])
    full_prompt = f"Context:\n{state['context']}\n\nTask:\n{task_prompt}"

    modifications = claude_api.generate_response(full_prompt)

    state["modifications"] = modifications

    return state







#File ./src/autocoder/nodes/initialize_node.py:
from typing import Dict
from langchain_core.tools import Tool
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel

class InitializeArgs(BaseModel):
    pass

def initialize(state: Dict, args: InitializeArgs) -> Dict:
    # Perform any necessary initialization
    state["initialized"] = True
    return state

initialize_tools = [
    Tool.from_function(
        func=initialize,
        name="initialize",
        description="Initialize the Autocoder system",
        args_schema=InitializeArgs
    )
]

initialize_node = ToolNode(initialize_tools)







#File ./src/autocoder/nodes/interpret_task.py:
from ..state import State
from ..task_interpreter import TaskInterpreter
from ..error_handler import ErrorHandler

@ErrorHandler.wrap_node
def interpret_task(state: State) -> State:
    task_interpreter = TaskInterpreter()
    task_description = state["messages"][-1].content
    interpreted_task = task_interpreter.interpret_task(task_description)
    state["interpreted_task"] = interpreted_task
    return state







#File ./src/autocoder/nodes/run_tests.py:
from ..state import State
from ..test_runner import TestRunner
from ..error_handler import ErrorHandler


@ErrorHandler.wrap_node
def run_tests(state: State) -> State:
    test_runner = TestRunner()

    success, test_result = test_runner.run_tests()

    state["test_results"] = {
        "success": success,
        "details": test_result
    }

    return state







#File ./src/autocoder/nodes/task_execution_node.py:
# File: src/autocoder/nodes/task_execution_node.py

from typing import Dict
from langchain_core.tools import Tool
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel, Field
from functools import partial


class TaskExecutionArgs(BaseModel):
    task_description: str = Field(..., description="Description of the task to execute")


def execute_task(state: Dict, args: TaskExecutionArgs, claude_api) -> Dict:
    try:
        # Build the prompt using the task description and context
        prompt = f"{HUMAN_PROMPT} Based on the following context, please perform the task: {args.task_description}\n\nContext:\n{state['context']}\n{AI_PROMPT}"

        # Call the LLM
        response = claude_api.completions.create(
            model="claude-instant-v1",
            prompt=prompt,
            max_tokens_to_sample=1000,
            stop_sequences=[HUMAN_PROMPT]
        )

        # Process the response
        state["task_result"] = response.completion.strip()
        state["task_completed"] = True
        return state
    except Exception as e:
        state["error"] = f"Error during task execution: {str(e)}"
        return state


def create_task_execution_node(claude_api):
    execute_task_partial = partial(execute_task, claude_api=claude_api)
    task_execution_tools = [
        Tool.from_function(
            func=execute_task_partial,
            name="execute_task",
            description="Execute the given task",
            args_schema=TaskExecutionArgs
        )
    ]
    return ToolNode(task_execution_tools)







#File ./src/autocoder/nodes/analyze_file_listing_node.py:
import os
import logging
from pathlib import Path
import pathspec
from typing import Dict, List
from langchain_core.tools import Tool
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel, Field
from langchain_core.messages import AIMessage

logger = logging.getLogger(__name__)

class AnalyzeFileListingArgs(BaseModel):
    project_root: str = Field(..., description="Root directory of the project")

class AnalyzeFileListingNode:
    @staticmethod
    def process(state: Dict, args: AnalyzeFileListingArgs) -> Dict:
        try:
            project_root = Path(args.project_root)
            ignore_spec = AnalyzeFileListingNode.get_ignore_spec(project_root)
            project_files = AnalyzeFileListingNode.list_project_files(project_root, ignore_spec)
            context = AnalyzeFileListingNode.build_context(project_root, project_files)

            state.update({
                'project_files': project_files,
                'excluded_files': [str(pat) for pat in ignore_spec.patterns],
                'context': context,
                'messages': state.get('messages', []) + [AIMessage(content="File listing and context building completed.")]
            })

            logger.info(f"File listing completed for analysis. Found {len(project_files)} files.")
            return state
        except Exception as e:
            logger.error(f"Error in AnalyzeFileListingNode: {str(e)}")
            state['error'] = str(e)
            state['messages'] = state.get('messages', []) + [AIMessage(content=f"Error: {str(e)}")]
            return state

    @staticmethod
    def get_ignore_spec(project_root: Path) -> pathspec.PathSpec:
        patterns = []
        gitignore_path = project_root / '.gitignore'
        if gitignore_path.exists():
            with gitignore_path.open('r') as f:
                patterns.extend(f.read().splitlines())
            logger.info("Read .gitignore and compiled ignore patterns.")
        else:
            logger.warning(".gitignore file not found.")

        default_ignores = [
            '.git/', '.hg/', '.svn/', '.idea/', '*.egg-info/',
            '__pycache__/', '.DS_Store', '*.pyc', '.venv/',
            'env/', 'build/', 'dist/', 'node_modules/',
            '*.log', '*.tmp',
        ]
        patterns.extend(default_ignores)
        return pathspec.PathSpec.from_lines('gitwildmatch', patterns)

    @staticmethod
    def list_project_files(project_root: Path, ignore_spec: pathspec.PathSpec) -> List[str]:
        project_files = []
        for root, dirs, files in os.walk(project_root):
            root_relative = os.path.relpath(root, project_root)
            if root_relative == '.':
                root_relative = ''
            dirs[:] = [d for d in dirs if not ignore_spec.match_file(os.path.join(root_relative, d) + '/')]
            for file in files:
                rel_path = os.path.join(root_relative, file)
                if not ignore_spec.match_file(rel_path):
                    project_files.append(rel_path)
        return project_files

    @staticmethod
    def build_context(project_root: Path, project_files: List[str]) -> str:
        context = "Project Files:\n"
        context += "\n".join(project_files)
        context += "\n\nFile Contents:\n"

        for file in project_files[:10]:  # Limit to first 10 files to avoid overwhelming the LLM
            file_path = project_root / file
            try:
                with file_path.open('r', encoding='utf-8') as f:
                    content = f.read()
                context += f'\n\n#File {file}:\n{content[:1000]}'  # Limit each file to first 1000 characters
            except Exception as e:
                logger.warning(f"Could not read file {file}: {str(e)}")

        return context


def analyze_file_listing(state: Dict, args: AnalyzeFileListingArgs) -> Dict:
    return AnalyzeFileListingNode.process(state, args)


analyze_file_listing_tools = [
    Tool.from_function(
        func=analyze_file_listing,
        name="analyze_file_listing",
        description="List project files and build context for analysis",
        args_schema=AnalyzeFileListingArgs
    )
]

analyze_file_listing_node = ToolNode(analyze_file_listing_tools)







#File ./src/autocoder/file_listing/__init__.py:
from .file_listing_node import FileListingNode






#File ./src/autocoder/file_listing/file_listing_node.py:
import os
from pathlib import Path
import logging
import pathspec

logger = logging.getLogger(__name__)

class FileListingNode:
    def __init__(self, project_root, claude_api):
        self.project_root = Path(project_root)
        self.claude_api = claude_api

    def process(self, state):
        try:
            ignore_spec = self.get_ignore_spec()
            project_files = self.list_project_files(ignore_spec)
            state['project_files'] = project_files
            state['excluded_files'] = [str(pat) for pat in ignore_spec.patterns]
            # Build context
            context = self.build_context(project_files)
            state['context'] = context
            logger.info("Context built successfully.")
            return state
        except Exception as e:
            logger.error(f"Error in FileListingNode: {str(e)}")
            state['error'] = str(e)
            return state

    def get_ignore_spec(self):
        patterns = []
        # Read .gitignore
        gitignore_path = self.project_root / '.gitignore'
        if gitignore_path.exists():
            with gitignore_path.open('r') as f:
                gitignore_content = f.read()
            patterns.extend(gitignore_content.splitlines())
            logger.info("Read .gitignore and compiled ignore patterns.")
        else:
            logger.warning(".gitignore file not found.")

        # Add default ignore patterns
        default_ignores = [
            '.git/',
            '.hg/',
            '.svn/',
            '.idea/',
            '*.egg-info/',
            '__pycache__/',
            '.DS_Store',
            '*.pyc',
            '.venv/',
            'env/',
            'build/',
            'dist/',
            'node_modules/',
            '*.log',
            '*.tmp',
        ]
        patterns.extend(default_ignores)
        spec = pathspec.PathSpec.from_lines('gitwildmatch', patterns)
        return spec

    def list_project_files(self, ignore_spec):
        project_files = []
        for root, dirs, files in os.walk(self.project_root):
            # Compute relative paths
            root_relative = os.path.relpath(root, self.project_root)
            if root_relative == '.':
                root_relative = ''
            # Prepare directory paths for matching
            dirs_to_remove = []
            for d in dirs:
                dir_path = os.path.join(root_relative, d)
                if ignore_spec.match_file(dir_path + '/'):
                    dirs_to_remove.append(d)
            for d in dirs_to_remove:
                dirs.remove(d)
            for file in files:
                rel_path = os.path.join(root_relative, file)
                if not ignore_spec.match_file(rel_path):
                    project_files.append(rel_path)
        logger.info(f"Listed project files: {len(project_files)} files found.")
        return project_files

    def build_context(self, project_files):
        context = ''
        for file in project_files:
            file_path = self.project_root / file
            try:
                with file_path.open('r', encoding='utf-8') as f:
                    content = f.read()
                context += f'#File {file}:\n{content}\n\n'
            except Exception as e:
                logger.warning(f"Could not read file {file}: {str(e)}")
        logger.info("Built context from project files.")
        return context







#File ./src/autocoder/__init__.py:
from .autocoder import main

from .file_manager import file_manager_node
from .context_builder import context_builder_node
from .task_interpreter import task_interpreter_node
from .code_modifier import code_modifier_node
from .test_runner import test_runner_node
from .error_handler import ErrorHandler
from .claude_api_wrapper import ClaudeAPIWrapper
from .langgraph_workflow import LangGraphWorkflow
from .state import State
from .file_listing.file_listing_node import FileListingNode

__all__ = [
    'main',
    'file_manager_node',
    'context_builder_node',
    'task_interpreter_node',
    'code_modifier_node',
    'test_runner_node',
    'ErrorHandler',
    'ClaudeAPIWrapper',
    'LangGraphWorkflow',
    'State',
    'FileListingNode'
]







#File ./src/autocoder/autocoder.py:
import argparse
import logging
import os
from dotenv import load_dotenv
from typing import Dict, Any

from .langgraph_workflow import LangGraphWorkflow
from .nodes.tools.directory_checker import (
    check_autocoder_dir,
    display_init_message,
    init_autocoder,
    display_usage_message,
)
from .error_handler import ErrorHandler

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def initialize_autocoder():
    logger.info("Starting Autocoder initialization...")
    init_autocoder()
    logger.info("Autocoder initialization complete.")
    return True

def stream_execution(workflow: LangGraphWorkflow, task_description: str, config: Dict[str, Any]):
    try:
        for event in workflow.graph.stream({
            "messages": [{"role": "user", "content": task_description}],
            "project_root": config.get("project_root", ""),
            "files": {},
            "context": "",
            "task_completed": False,
            "error": None
        }, config):
            if "error" in event:
                yield f"An error occurred: {event['error']}"
            elif "messages" in event:
                yield f"Output: {event['messages'][-1]['content']}"
            else:
                yield f"Event: {event}"
    except Exception as e:
        error_report = ErrorHandler.handle_error(e)
        ErrorHandler.log_error(e)
        yield f"An unexpected error occurred: {error_report['error_message']}"

def execute_task(task_description):
    if not check_autocoder_dir():
        logger.error("Autocoder is not initialized in this directory.")
        print("Autocoder is not initialized in this directory. Please run 'autocoder init' first.")
        return

    load_dotenv()
    api_key = os.getenv('ANTHROPIC_API_KEY') or os.getenv('CLAUDE_API_KEY')
    if not api_key:
        logger.error("No API key found. Cannot proceed with task execution.")
        print("Error: No API key found. Please set ANTHROPIC_API_KEY or CLAUDE_API_KEY in your environment or .env file.")
        return

    try:
        workflow = LangGraphWorkflow(api_key)
        print("Executing task. Streaming output:")
        for output in stream_execution(workflow, task_description, {"project_root": os.getcwd()}):
            print(output)
    except Exception as e:
        logger.error(f"Failed to execute task: {str(e)}")
        print(f"Error: Failed to execute task: {str(e)}")

def execute_analyze():
    if not check_autocoder_dir():
        logger.error("Autocoder is not initialized in this directory.")
        print(
            "Autocoder is not initialized in this directory. Please run 'autocoder init' first."
        )
        return

    load_dotenv()
    api_key = os.getenv("ANTHROPIC_API_KEY") or os.getenv("CLAUDE_API_KEY")
    if not api_key:
        logger.error("No API key found. Cannot proceed with analysis.")
        print(
            "Error: No API key found. Please set ANTHROPIC_API_KEY or CLAUDE_API_KEY in your environment or .env file."
        )
        return

    try:
        workflow = LangGraphWorkflow(api_key)
        print("Analyzing project...")
        result = workflow.execute_analysis({"project_root": os.getcwd()})
        if result != "Analysis completed.":
            print(result)
    except Exception as e:
        logger.error(f"Failed to execute analysis: {str(e)}")
        print(f"Error: Failed to execute analysis: {str(e)}")

def main():
    parser = argparse.ArgumentParser(description="Claude Automated Coding")
    parser.add_argument(
        "command",
        nargs="?",
        default="help",
        choices=["init", "task", "analyze", "help"],
        help="Command to execute",
    )
    parser.add_argument(
        "task_description",
        nargs="?",
        default="",
        help="The task description for the automated coding process",
    )
    args = parser.parse_args()

    logger.debug(f"Received command: {args.command}")

    if args.command == "init":
        logger.info("Initializing Autocoder...")
        if not initialize_autocoder():
            return
    elif args.command == "task":
        if args.task_description:
            logger.info(f"Executing task: {args.task_description}")
            execute_task(args.task_description)
        else:
            logger.error("No task description provided for 'task' command.")
            print("Error: Task description is required for the 'task' command.")
            display_usage_message()
    elif args.command == "analyze":
        logger.info("Analyzing project...")
        execute_analyze()
    elif args.command == "help" or not args.command:
        if check_autocoder_dir():
            logger.info("Displaying usage message for initialized directory.")
            display_usage_message()
        else:
            logger.info(
                "Displaying initialization message for uninitialized directory."
            )
            display_init_message()
    else:
        logger.error(f"Unknown command: {args.command}")
        print(f"Unknown command: {args.command}")
        display_usage_message()

if __name__ == "__main__":
    main()







#File ./src/autocoder/claude_api_wrapper.py:
from anthropic import Anthropic
from typing import Dict, Any, List
from langchain_core.tools import Tool
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel, Field

class GenerateResponseArgs(BaseModel):
    messages: List[Dict[str, str]] = Field(..., description="List of messages in the conversation")
    max_tokens: int = Field(1000, description="Maximum number of tokens in the response")
    system_prompt: str = Field("You are a helpful AI assistant.", description="System prompt to guide the model's behavior")

class ClaudeAPIWrapper:
    def __init__(self, api_key: str):
        self.client = Anthropic(api_key=api_key)
        self.model = "claude-3-opus-20240229"

    def generate_response(self, state: Dict[str, Any], args: GenerateResponseArgs) -> Dict[str, Any]:
        try:
            messages = [{"role": "system", "content": args.system_prompt}] + args.messages

            response = self.client.messages.create(
                model=self.model,
                max_tokens=args.max_tokens,
                messages=messages
            )
            return {"response": response.content[0].text}
        except Exception as e:
            return {"error": f"An error occurred: {str(e)}"}

claude_api_tools = [
    Tool.from_function(
        func=ClaudeAPIWrapper.generate_response,
        name="generate_response",
        description="Generate a response using the Claude API",
        args_schema=GenerateResponseArgs
    )
]

claude_api_node = ToolNode(claude_api_tools)







#File ./src/autocoder/code_modifier.py:
import ast
import astor
from typing import Dict
from langchain_core.tools import Tool
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel, Field

class CodeModifierArgs(BaseModel):
    original_code: str = Field(..., description="The original code to be modified")
    modifications: str = Field(..., description="Description of modifications to be made")

def code_modifier(state: Dict, args: CodeModifierArgs) -> Dict:
    try:
        tree = ast.parse(args.original_code)
        modified_tree = apply_modifications(tree, args.modifications)
        modified_code = astor.to_source(modified_tree)
        return {"modified_code": modified_code}
    except SyntaxError:
        return {"modified_code": simple_modify(args.original_code, args.modifications)}

def apply_modifications(tree, modifications):
    # TODO: Implement more sophisticated AST transformations here
    new_node = ast.Expr(ast.Str(f"# Modified: {modifications}"))
    tree.body.insert(0, new_node)
    return tree

def simple_modify(original_code, modifications):
    return f"# Modified: {modifications}\n\n{original_code}"

code_modifier_tools = [
    Tool.from_function(
        func=code_modifier,
        name="code_modifier",
        description="Modify code based on given instructions",
        args_schema=CodeModifierArgs
    )
]

code_modifier_node = ToolNode(code_modifier_tools)







#File ./src/autocoder/context_builder.py:
from typing import Dict, List
from langchain_core.tools import Tool
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel, Field
from .file_manager import read_file, ReadFileArgs

class BuildContextArgs(BaseModel):
    files: List[str] = Field(..., description="List of file paths to build context from")

def build_context(state: Dict, args: BuildContextArgs) -> Dict:
    context = ""
    for file in args.files:
        content = read_file(state, ReadFileArgs(file_path=file))["content"]
        context += f"#File {file}:\n{content}\n\n"
    return {"context": context}

context_builder_tools = [
    Tool.from_function(
        func=build_context,
        name="build_context",
        description="Build context from a list of files",
        args_schema=BuildContextArgs
    )
]

context_builder_node = ToolNode(context_builder_tools)







#File ./src/autocoder/error_handler.py:
import traceback
import logging
from typing import Union, Dict, Any

logger = logging.getLogger(__name__)

class ErrorHandler:
    @staticmethod
    def handle_error(error: Union[str, Exception], context: Dict[str, Any] = None) -> Dict[str, Any]:
        if isinstance(error, str):
            error_message = error
        else:
            error_message = str(error)

        logger.error(f"An error occurred: {error_message}")

        error_report = {
            "error_message": error_message,
            "traceback": ErrorHandler.get_traceback(error),
            "context": context or {},
            "suggestions": ErrorHandler.get_suggestions(error_message)
        }

        return error_report

    @staticmethod
    def get_traceback(error: Union[str, Exception]) -> str:
        if isinstance(error, Exception):
            return "".join(traceback.format_exception(type(error), error, error.__traceback__))
        return "No traceback available for string errors."

    @staticmethod
    def get_suggestions(error_message: str) -> list:
        # This method can be expanded with more specific suggestions based on error types
        suggestions = [
            "Check the error message and traceback for details on where the error occurred.",
            "Review any recent changes to the affected components.",
            "Ensure all necessary dependencies are installed and up to date.",
            "If the error persists, consider reverting recent changes or seeking further assistance."
        ]
        return suggestions

    @staticmethod
    def log_error(error: Union[str, Exception], context: Dict[str, Any] = None):
        error_report = ErrorHandler.handle_error(error, context)
        logger.error(f"Error Report: {error_report}")

    @staticmethod
    def wrap_node(node_func):
        def wrapped_node(state: Dict[str, Any]) -> Dict[str, Any]:
            try:
                return node_func(state)
            except Exception as e:
                error_report = ErrorHandler.handle_error(e, context={"node": node_func.__name__, "state": state})
                state["error"] = error_report
                return state
        return wrapped_node







#File ./src/autocoder/file_manager.py:
import os
from typing import Dict, List
from langchain_core.tools import Tool
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel, Field

class ReadFileArgs(BaseModel):
    file_path: str = Field(..., description="Path to the file to read")

class WriteFileArgs(BaseModel):
    file_path: str = Field(..., description="Path to the file to write")
    content: str = Field(..., description="Content to write to the file")

def read_file(state: Dict, args: ReadFileArgs) -> Dict:
    project_root = state.get("project_root", "")
    with open(os.path.join(project_root, args.file_path), 'r') as file:
        content = file.read()
    return {"content": content}

def write_file(state: Dict, args: WriteFileArgs) -> Dict:
    project_root = state.get("project_root", "")
    with open(os.path.join(project_root, args.file_path), 'w') as file:
        file.write(args.content)
    return {"status": "success"}

def list_files(state: Dict) -> Dict:
    project_root = state.get("project_root", "")
    file_list = []
    for root, _, files in os.walk(project_root):
        for file in files:
            file_list.append(os.path.relpath(os.path.join(root, file), project_root))
    return {"files": file_list}

file_manager_tools = [
    Tool.from_function(
        func=read_file,
        name="read_file",
        description="Read the contents of a file",
        args_schema=ReadFileArgs
    ),
    Tool.from_function(
        func=write_file,
        name="write_file",
        description="Write content to a file",
        args_schema=WriteFileArgs
    ),
    Tool.from_function(
        func=list_files,
        name="list_files",
        description="List all files in the project"
    )
]

file_manager_node = ToolNode(file_manager_tools)







#File ./src/autocoder/langgraph_workflow.py:
import logging
from typing import Dict, Any
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from .state import State
from .nodes.file_listing_node import file_listing_node
from .nodes.initialize_node import initialize_node
from .nodes.error_handling_node import error_handling_node
from .claude_api_wrapper import ClaudeAPIWrapper
from .nodes.task_execution_node import create_task_execution_node
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage

# Import new nodes
from .nodes.analyze_file_listing_node import analyze_file_listing_node
from .nodes.llm_analyze_node import create_llm_analyze_node

logger = logging.getLogger(__name__)


class LangGraphWorkflow:
    def __init__(self, api_key: str):
        self.claude_api = ClaudeAPIWrapper(api_key).client
        self.graph = self._build_graph()
        self.analyze_graph = self._build_analyze_graph()
        self.memory = MemorySaver()

    def _build_graph(self) -> StateGraph:
        workflow = StateGraph(State)

        # Define nodes
        workflow.add_node("initialize", initialize_node)
        workflow.add_node("file_listing", file_listing_node)
        workflow.add_node("task_execution", create_task_execution_node(self.claude_api))
        workflow.add_node("error_handling", error_handling_node)

        # Define edges
        workflow.set_entry_point("initialize")
        workflow.add_edge("initialize", "file_listing")
        workflow.add_edge("file_listing", "task_execution")
        workflow.add_conditional_edges(
            "task_execution",
            self._handle_task_execution_result,
            {True: END, False: "error_handling"}
        )
        workflow.add_edge("error_handling", "task_execution")

        return workflow.compile()

    def _build_analyze_graph(self) -> StateGraph:
        workflow = StateGraph(State)

        # Define nodes
        workflow.add_node("analyze_file_listing", analyze_file_listing_node)
        workflow.add_node("llm_analyze", create_llm_analyze_node(self.claude_api))

        # Define edges
        workflow.set_entry_point("analyze_file_listing")
        workflow.add_edge("analyze_file_listing", "llm_analyze")

        # Add a conditional edge to end the workflow
        workflow.add_conditional_edges(
            "llm_analyze",
            self._handle_analysis_result,
            {
                True: END,
                False: "analyze_file_listing"  # Loop back if analysis is not complete
            }
        )

        return workflow.compile()

    def execute_analysis(self, config: Dict[str, Any] = None) -> str:
        try:
            initial_state = State(
                messages=[AIMessage(content="Please analyze the project.")],
                project_root=config.get("project_root", ""),
                context="",
                analysis_result="",
                error=None
            )
            logger.info(f"Initial state: {initial_state}")

            # Set a higher recursion limit
            config = config or {}
            config['recursion_limit'] = 50  # Adjust this value as needed

            for event in self.analyze_graph.stream(initial_state, config):
                logger.info(f"Event received: {event}")
                if "error" in event:
                    logger.error(f"Error in event: {event['error']}")
                    return f"An error occurred: {event['error']}"
                elif "messages" in event:
                    logger.debug(f"Received messages: {event['messages']}")
                    for i, message in enumerate(event["messages"]):
                        logger.debug(f"Message {i} type: {type(message)}")
                        if not isinstance(message, BaseMessage):
                            logger.warning(f"Message {i} is not a BaseMessage instance: {type(message)}")
                            if isinstance(message, dict):
                                content = message.get('content', str(message))
                                role = message.get('role', 'AI')
                                if role.lower() == 'human':
                                    event["messages"][i] = HumanMessage(content=content)
                                elif role.lower() == 'system':
                                    event["messages"][i] = SystemMessage(content=content)
                                else:
                                    event["messages"][i] = AIMessage(content=content)
                            else:
                                event["messages"][i] = AIMessage(content=str(message))
                    last_message = event["messages"][-1]
                    logger.info(f"Last message type: {type(last_message)}")
                    logger.info(f"Last message: {last_message.content}")
                elif "analysis_result" in event:
                    logger.info("Received analysis result")
                    print("LLM Analysis of the Project:")
                    print(event["analysis_result"])
                    return "Analysis completed."

            logger.warning("No analysis result was generated")
            return "Analysis completed with no result."
        except Exception as e:
            logger.exception(f"An unexpected error occurred during analysis: {str(e)}")
            return f"An unexpected error occurred during analysis: {str(e)}"

    def _handle_analysis_result(self, state: State) -> bool:
        # Check if we have an analysis result and it's not empty
        if 'analysis_result' in state and state['analysis_result'].strip():
            logger.info("Analysis result found. Ending the workflow.")
            return True
        elif state.get('error'):
            logger.error(f"Error encountered: {state['error']}. Ending the workflow.")
            return True
        else:
            logger.info("No analysis result yet. Continuing the workflow.")
            return False

    def _handle_task_execution_result(self, state: State) -> bool:
        """
        Determines whether the task execution is complete based on the current state.

        Args:
        state (State): The current state of the workflow.

        Returns:
        bool: True if the task is completed, False otherwise.
        """
        logger.debug(f"Handling task execution result. Current state: {state}")

        # Check if there's an error in the state
        if state.get("error"):
            logger.error(f"Error encountered during task execution: {state['error']}")
            return False

        # Check if the task is marked as completed
        if state.get("task_completed"):
            logger.info("Task marked as completed.")
            return True

        # Check if we have a final result or output
        if state.get("final_result") or state.get("output"):
            logger.info("Final result or output found. Considering task as completed.")
            return True

        # Check if we've reached a maximum number of iterations
        max_iterations = state.get("max_iterations", 5)  # Default to 5 if not set
        current_iteration = state.get("current_iteration", 0)
        if current_iteration >= max_iterations:
            logger.warning(f"Reached maximum iterations ({max_iterations}). Ending task execution.")
            return True

        # If none of the above conditions are met, continue the task execution
        logger.info("Task execution continuing.")
        return False

    def execute(self, task_description: str, config: Dict[str, Any] = None) -> str:
        try:
            initial_state = State(
                messages=[HumanMessage(content=task_description)],
                project_root=config.get("project_root", ""),
                claude_api=self.claude_api,
                files={},
                context="",
                task_completed=False,
                error=None
            )
            for event in self.graph.stream(initial_state, config):
                if "error" in event:
                    return f"An error occurred: {event['error']}"
                if "messages" in event:
                    for i, message in enumerate(event["messages"]):
                        if not isinstance(message, BaseMessage):
                            if isinstance(message, dict):
                                content = message.get('content', str(message))
                                role = message.get('role', 'AI')
                                if role.lower() == 'human':
                                    event["messages"][i] = HumanMessage(content=content)
                                elif role.lower() == 'system':
                                    event["messages"][i] = SystemMessage(content=content)
                                else:
                                    event["messages"][i] = AIMessage(content=content)
                            else:
                                event["messages"][i] = AIMessage(content=str(message))
                    last_message = event["messages"][-1]
                    print(f"AI: {last_message.content}")
            return "Task execution completed."
        except Exception as e:
            logger.exception(f"An unexpected error occurred: {str(e)}")
            return f"An unexpected error occurred: {str(e)}"







#File ./src/autocoder/state.py:
from typing import Annotated, TypedDict, List, Dict
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage

class State(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]  # Ensure this line exists
    files: Dict[str, str]
    context: str
    interpreted_task: Dict[str, any]
    modifications: str
    test_results: str
    project_root: str
    autocoder_dir_exists: bool







#File ./src/autocoder/task_interpreter.py:
from typing import Dict
from enum import Enum
import re
from langchain_core.tools import Tool
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel, Field

class TaskType(Enum):
    ADD_FEATURE = "add_feature"
    FIX_BUG = "fix_bug"
    REFACTOR = "refactor"
    OPTIMIZE = "optimize"
    TEST = "test"
    DOCUMENT = "document"
    UNKNOWN = "unknown"

class TaskInterpreterArgs(BaseModel):
    task_description: str = Field(..., description="Description of the task to interpret")

def task_interpreter(state: Dict, args: TaskInterpreterArgs) -> Dict:
    task_type_keywords = {
        TaskType.ADD_FEATURE: ["add", "create", "implement", "new feature"],
        TaskType.FIX_BUG: ["fix", "bug", "issue", "problem", "error"],
        TaskType.REFACTOR: ["refactor", "restructure", "reorganize"],
        TaskType.OPTIMIZE: ["optimize", "improve performance", "speed up"],
        TaskType.TEST: ["test", "unit test", "integration test"],
        TaskType.DOCUMENT: ["document", "add comments", "explain"]
    }

    task_type = TaskType.UNKNOWN
    for t_type, keywords in task_type_keywords.items():
        if any(keyword in args.task_description.lower() for keyword in keywords):
            task_type = t_type
            break

    file_pattern = r'\b[\w-]+\.(py|js|html|css|md)\b'
    affected_files = list(set(re.findall(file_pattern, args.task_description)))

    subtasks = [subtask.strip() for subtask in args.task_description.split(". ") if subtask.strip()]

    return {
        "task_type": task_type.value,
        "affected_files": affected_files,
        "subtasks": subtasks
    }

task_interpreter_tools = [
    Tool.from_function(
        func=task_interpreter,
        name="task_interpreter",
        description="Interpret a task description",
        args_schema=TaskInterpreterArgs
    )
]

task_interpreter_node = ToolNode(task_interpreter_tools)







#File ./src/autocoder/test_runner.py:
import subprocess
from typing import Dict
from langchain_core.tools import Tool
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel

class TestRunnerArgs(BaseModel):
    pass  # No arguments needed for test runner

def test_runner(state: Dict, args: TestRunnerArgs) -> Dict:
    try:
        result = subprocess.run(['pytest'], capture_output=True, text=True)
        if result.returncode == 0:
            return {"success": True, "details": "All tests passed"}
        else:
            return {"success": False, "details": result.stderr}
    except Exception as e:
        return {"success": False, "details": f"Error running tests: {str(e)}"}

test_runner_tools = [
    Tool.from_function(
        func=test_runner,
        name="test_runner",
        description="Run tests for the project",
        args_schema=TestRunnerArgs
    )
]

test_runner_node = ToolNode(test_runner_tools)







#File ./tests/test_context_builder.py:
import pytest
import os
from autocoder.context_builder import ContextBuilder
from autocoder.file_manager import FileManager


@pytest.fixture
def project_root():
    return os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


@pytest.fixture
def file_manager(project_root):
    return FileManager(project_root)


@pytest.fixture
def context_builder():
    return ContextBuilder()


def test_build_context_with_manifest(context_builder, file_manager):
    context = context_builder.build_context(file_manager)

    print("\nBuild Context Output (Based on MANIFEST.in):")
    print(context)
    print("End of Build Context Output")

    assert "Project Files:" in context

    # Check for some expected files (adjust these based on your project structure)
    assert "File: src/autocoder/context_builder.py" in context
    assert "File: src/autocoder/file_manager.py" in context
    assert "File: src/autocoder/manifest_processor.py" in context

    # Check that the content of files is included
    assert "class ContextBuilder:" in context
    assert "class FileManager:" in context
    assert "class ManifestProcessor:" in context


def test_context_builder_methods(context_builder):
    # Test adding and getting context
    context_builder.add_context("key1", "value1")
    assert context_builder.get_context("key1") == "value1"

    # Test updating context
    context_builder.update_context("key1", "new_value1")
    assert context_builder.get_context("key1") == "new_value1"

    # Test removing context
    context_builder.remove_context("key1")
    with pytest.raises(KeyError):
        context_builder.get_context("key1")

    # Test clearing context
    context_builder.add_context("key2", "value2")
    context_builder.clear_context()
    assert context_builder.get_size() == 0

    # Test context existence
    context_builder.add_context("key3", "value3")
    assert context_builder.context_exists("key3")
    assert not context_builder.context_exists("key4")

    # Test getting size
    assert context_builder.get_size() == 1

    # Test adding multiple context
    context_builder.add_multiple_context({"key4": "value4", "key5": "value5"})
    assert context_builder.get_size() == 3


def test_get_full_context(context_builder):
    context_builder.add_multiple_context({"key1": "value1", "key2": "value2"})
    full_context = context_builder.get_full_context()
    assert full_context == {"key1": "value1", "key2": "value2"}






#File ./tests/test_file_listing_node.py:
import os
import logging
from autocoder.file_listing.file_listing_node import FileListingNode
from autocoder.claude_api_wrapper import ClaudeAPIWrapper

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_file_listing_node():
    project_root = os.getcwd()  # Or specify the path to your project
    api_key = 'your_api_key_here'  # Replace with your actual API key
    claude_api = ClaudeAPIWrapper(api_key)
    file_lister = FileListingNode(project_root, claude_api)

    state = {"project_root": project_root, "claude_api": claude_api}
    updated_state = file_lister.process(state)

    if 'error' in updated_state:
        print(f"Error: {updated_state['error']}")
    else:
        print("Project Files:")
        for file in updated_state['project_files']:
            print(file)
        print("\nContext:")
        print(updated_state['context'][:500])  # Print first 500 characters of context

if __name__ == "__main__":
    test_file_listing_node()







#File ./.env.example:
PROJECT_DIRECTORY=/path/to/your/project
CLAUDE_API_KEY=your_claude_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
DEBUG=true







#File ./requirements.txt:
python-dotenv==1.0.1
requests==2.32.3
openai==1.36
anthropic==0.31.2
langgraph==0.1.9
pytest==8.2.2
astor==0.8.1
pathspec==0.12.1







#File ./files:
./conftest.py
./src/autocoder/nodes/tools/__init__.py
./src/autocoder/nodes/tools/directory_checker.py
./src/autocoder/nodes/__init__.py
./src/autocoder/nodes/apply_modifications.py
./src/autocoder/nodes/build_context.py
./src/autocoder/nodes/check_autocoder_dir.py
./src/autocoder/nodes/error_handling_node.py
./src/autocoder/nodes/file_listing_node.py
./src/autocoder/nodes/generate_modifications.py
./src/autocoder/nodes/initialize_node.py
./src/autocoder/nodes/interpret_task.py
./src/autocoder/nodes/run_tests.py
./src/autocoder/nodes/task_execution_node.py
./src/autocoder/nodes/analyze_file_listing_node.py
./src/autocoder/file_listing/__init__.py
./src/autocoder/file_listing/file_listing_node.py
./src/autocoder/__init__.py
./src/autocoder/autocoder.py
./src/autocoder/claude_api_wrapper.py
./src/autocoder/code_modifier.py
./src/autocoder/context_builder.py
./src/autocoder/error_handler.py
./src/autocoder/file_manager.py
./src/autocoder/langgraph_workflow.py
./src/autocoder/state.py
./src/autocoder/task_interpreter.py
./src/autocoder/test_runner.py
./tests/test_context_builder.py
./tests/test_file_listing_node.py
./.env.example
./requirements.txt
./files







